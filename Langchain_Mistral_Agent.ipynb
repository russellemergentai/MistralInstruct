{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/russellemergentai/MistralInstruct/blob/main/Langchain_Mistral_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#login\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Install required packages\n",
        "!pip install langchain\n",
        "!pip install langchain-community\n",
        "!pip install langchain-chroma\n",
        "!pip install langchain-huggingface\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes\n",
        "!pip install wikipedia\n",
        "\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import core libraries and dependencies\n",
        "import numexpr as ne\n",
        "import torch, os, uuid\n",
        "from typing import Optional, List, Mapping, Any\n",
        "\n",
        "# Import transformers models and utilities\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "from transformers.models.mistral.modeling_mistral import MistralForCausalLM\n",
        "from transformers.models.llama.tokenization_llama_fast import LlamaTokenizerFast\n",
        "\n",
        "# Import LangChain modules and utilities\n",
        "from langchain.tools import WikipediaQueryRun, BaseTool\n",
        "from langchain.agents import Tool\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.agents import create_json_chat_agent, AgentExecutor\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_chroma import Chroma\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.storage import InMemoryByteStore\n",
        "from pathlib import Path\n",
        "\n",
        "# load model and tokenizer\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, quantization_config=quantization_config, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# wrap the LLNM\n",
        "class CustomLLMMistral(LLM):\n",
        "    model: MistralForCausalLM\n",
        "    tokenizer: LlamaTokenizerFast\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom\"\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None) -> str:\n",
        "\n",
        "        messages = [\n",
        "         {\"role\": \"user\", \"content\": prompt},\n",
        "        ]\n",
        "\n",
        "        encodeds = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "        model_inputs = encodeds.to(self.model.device)\n",
        "\n",
        "        generated_ids = self.model.generate(model_inputs, max_new_tokens=512, do_sample=True,\n",
        "                                            pad_token_id=self.tokenizer.eos_token_id, top_k=4, temperature=0.7)\n",
        "\n",
        "        decoded = self.tokenizer.batch_decode(generated_ids)\n",
        "\n",
        "        output = decoded[0].split(\"[/INST]\")[1].replace(\"</s>\", \"\").strip()\n",
        "\n",
        "        if stop is not None:\n",
        "          for word in stop:\n",
        "            output = output.split(word)[0].strip()\n",
        "\n",
        "        # Mistral 7B sometimes fails to properly close the Markdown Snippets.\n",
        "        # If they are not correctly closed, Langchain will struggle to parse the output.\n",
        "        while not output.endswith(\"```\"):\n",
        "          output += \"`\"\n",
        "\n",
        "        return output\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {\"model\": self.model}\n",
        "\n",
        "\n",
        "llm = CustomLLMMistral(model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "h5ARqsLzLM_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tools"
      ],
      "metadata": {
        "id": "1-fsPHA4hfBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=2500))\n",
        "\n",
        "wikipedia_tool = Tool(\n",
        "    name=\"wikipedia\",\n",
        "    description=\"Never search for more than one concept at a single step. If you need to compare two concepts, search for each one individually. Syntax: string with a simple concept\",\n",
        "    func=wikipedia.run\n",
        ")\n",
        "\n",
        "class Calculator(BaseTool):\n",
        "    name: str = \"calculator\"\n",
        "    description: str = \"Use this tool for math operations. It requires numexpr syntax. Use it always you need to solve any math operation. Be sure syntax is correct.\"\n",
        "\n",
        "    def _run(self, expression: str):\n",
        "      try:\n",
        "        return ne.evaluate(expression).item()\n",
        "      except Exception:\n",
        "        return \"This is not a numexpr valid syntax. Try a different syntax.\"\n",
        "\n",
        "    def _arun(self, radius: int):\n",
        "        raise NotImplementedError(\"This tool does not support async\")\n",
        "\n",
        "calculator_tool = Calculator()\n",
        "\n",
        "\n",
        "def create_multivector_directory_retriever(directory_path):\n",
        "\n",
        "    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=500) #A\n",
        "    child_splitter = RecursiveCharacterTextSplitter(chunk_size=250) #B\n",
        "\n",
        "    model_path = \"intfloat/e5-large-unsupervised\"\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "        model_name=model_path,\n",
        "        model_kwargs={'device': 'cuda'},\n",
        "        encode_kwargs={'normalize_embeddings': False}\n",
        "    )\n",
        "\n",
        "    child_chunks_collection = Chroma(\n",
        "        collection_name=\"uk_child_chunks\",\n",
        "        embedding_function=embeddings,\n",
        "    )\n",
        "\n",
        "    child_chunks_collection.reset_collection()\n",
        "\n",
        "    doc_byte_store = InMemoryByteStore()\n",
        "    doc_key = \"doc_id\"\n",
        "\n",
        "    multi_vector_retriever = MultiVectorRetriever(\n",
        "        vectorstore=child_chunks_collection,\n",
        "        byte_store=doc_byte_store\n",
        "    )\n",
        "\n",
        "    all_documents = []\n",
        "\n",
        "    for file_path in Path(directory_path).rglob('*'):\n",
        "        if file_path.is_file():\n",
        "            loader = TextLoader(str(file_path), encoding='UTF-8')\n",
        "            documents = loader.load()\n",
        "            all_documents.extend(documents)\n",
        "\n",
        "    coarse_chunks = parent_splitter.split_documents(all_documents)\n",
        "\n",
        "    coarse_chunks_ids = [str(uuid.uuid4()) for _ in coarse_chunks]\n",
        "    all_granular_chunks = []\n",
        "\n",
        "    for i, coarse_chunk in enumerate(coarse_chunks):\n",
        "        coarse_chunk_id = coarse_chunks_ids[i]\n",
        "        granular_chunks = child_splitter.split_documents([coarse_chunk])\n",
        "\n",
        "        for granular_chunk in granular_chunks:\n",
        "            granular_chunk.metadata[doc_key] = coarse_chunk_id\n",
        "            all_granular_chunks.extend(granular_chunks)\n",
        "\n",
        "    multi_vector_retriever.vectorstore.add_documents(all_granular_chunks)\n",
        "    multi_vector_retriever.docstore.mset(list(zip(coarse_chunks_ids, coarse_chunks)))\n",
        "\n",
        "    return multi_vector_retriever\n",
        "\n",
        "\n",
        "  # retrieve from data directory\n",
        "def retrieval_multivector_query_data(expression: str):\n",
        "\n",
        "  # It's important to note that to effectively prompt the Mistral 7B Instruct and get optimal outputs,\n",
        "  # it's recommended to use the following chat template:\n",
        "  # <s>[INST] Instruction [/INST] Model answer</s>[INST] Follow-up instruction [/INST]\n",
        "  prompt_template=\"\"\"\n",
        "  <s>\n",
        "  [INST]\n",
        "  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "  {query}\n",
        "  [/INST]\n",
        "  </s>\n",
        "  [INST]Keep your response succinct.[/INST]\n",
        "  \"\"\"\n",
        "\n",
        "  path=\"/content/drive/MyDrive/Target\"\n",
        "\n",
        "  retriever = create_multivector_directory_retriever(path)\n",
        "\n",
        "  common_params = {\n",
        "    'max_length': 512,\n",
        "    'eos_token_id': tokenizer.eos_token_id,\n",
        "  }\n",
        "\n",
        "  # Create the pipeline for text generation with output length constraint\n",
        "  pipelineQuery = pipeline(\n",
        "      \"text-generation\",\n",
        "      model=model,\n",
        "      tokenizer=tokenizer,\n",
        "      **common_params,\n",
        "      max_new_tokens=512\n",
        "  )\n",
        "\n",
        "  llmPipelineQuery = HuggingFacePipeline(pipeline=pipelineQuery, model_kwargs={\"temperature\": 0.1})\n",
        "  qa = RetrievalQA.from_chain_type(llm=llmPipelineQuery, retriever=retriever, return_source_documents=False)\n",
        "  result = qa.run({\"query\": expression})\n",
        "\n",
        "  del pipelineQuery\n",
        "  del llmPipelineQuery\n",
        "  del qa\n",
        "  del retriever\n",
        "  import gc\n",
        "  gc.collect()\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "class RAGQuery(BaseTool):\n",
        "    name: str = \"rag\"\n",
        "    description: str = \"Use this tool for retrieval augmented generation rag operations from my personal files. \\\n",
        "    Use it to always when rag is requested or the subject is: Murex; Summit; STF.\"\n",
        "\n",
        "    def _run(self, expression: str):\n",
        "      try:\n",
        "        return retrieval_multivector_query_data(expression)\n",
        "      except Exception as e:\n",
        "        s = f\"An exception occurred: {e}\"\n",
        "        return s\n",
        "\n",
        "    def _arun(self, radius: int):\n",
        "        raise NotImplementedError(\"This tool does not support async\")\n",
        "\n",
        "rag_tool = RAGQuery()\n",
        "\n",
        "tools = [wikipedia_tool, calculator_tool, rag_tool]\n"
      ],
      "metadata": {
        "id": "8qXMWtcPaNMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt"
      ],
      "metadata": {
        "id": "ieNdbNj9hakQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system=\"\"\"\n",
        "You are designed to solve tasks. Each task requires multiple steps that are represented by a markdown code snippet of a json blob.\n",
        "The json structure should contain the following keys:\n",
        "thought -> your thoughts\n",
        "action -> name of a tool\n",
        "action_input -> parameters to send to the tool\n",
        "\n",
        "These are the tools you can use: {tool_names}.\n",
        "\n",
        "These are the tools descriptions:\n",
        "\n",
        "{tools}\n",
        "\n",
        "If you have enough information to answer the query use the tool \"Final Answer\". Its parameters is the solution.\n",
        "If there is not enough information, keep trying.\n",
        "\"\"\"\n",
        "\n",
        "human=\"\"\"\n",
        "Add the word \"STOP\" after each markdown snippet. Example:\n",
        "\n",
        "```json\n",
        "{{\"thought\": \"<your thoughts>\",\n",
        " \"action\": \"<tool name or Final Answer to give a final answer>\",\n",
        " \"action_input\": \"<tool parameters or the final output\"}}\n",
        "```\n",
        "STOP\n",
        "\n",
        "This is my query=\"{input}\". Write only the next step needed to solve it.\n",
        "Your answer should be based in the previous tools executions, even if you think you know the answer.\n",
        "Remember to add STOP after each snippet.\n",
        "\n",
        "These were the previous steps given to solve this query and the information you already gathered:\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
        "        (\"human\", human),\n",
        "        MessagesPlaceholder(\"agent_scratchpad\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "8kI0wCpDMheZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agents"
      ],
      "metadata": {
        "id": "p6DkeQtdhojI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_json_chat_agent(\n",
        "    tools = tools,\n",
        "    llm = llm,\n",
        "    prompt = prompt,\n",
        "    stop_sequence = [\"STOP\"],\n",
        "    template_tool_response = \"{observation}\"\n",
        ")\n",
        "\n",
        "#memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "#agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True, memory=memory)\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
        "\n",
        "def main():\n",
        "\n",
        "    while True:\n",
        "        query = input(\"Enter query: \").lower()\n",
        "\n",
        "\n",
        "        if query==\"x\":\n",
        "            print(\"Exiting.\")\n",
        "            break\n",
        "\n",
        "        agent_executor.invoke({\"input\": query})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "HW03b22pUC5T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}